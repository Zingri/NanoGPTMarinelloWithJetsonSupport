# ü§ñ NanoGPT Marinello ‚Äî Interfaccia Conversazionale Locale

**NanoGPT Marinello** √® una versione minimalista di un modello GPT progettata per l‚Äôesecuzione **locale (CPU/GPU)** e su **dispositivi NVIDIA Jetson**.  
Fornisce un assistente conversazionale completo con interfaccia web, API REST e funzionalit√† di training personalizzato.

---

## ‚ú® Caratteristiche principali

- **Interfaccia web (Gradio)** per chat testuale e vocale  
- **API REST (Flask)** per integrazione remota e uso su Jetson  
- **Pipeline di training** semplice con salvataggio dei checkpoint  
- **Trascrizione vocale** tramite *Whisper* (opzionale)  
- **Text-To-Speech (TTS)** tramite *gTTS* (opzionale)  
- **Salvataggio cronologia conversazioni** in SQLite  
- **Adattamento automatico** dei parametri in base alle risorse di sistema  

---

## ‚öôÔ∏è Requisiti

### Dipendenze principali
- Python ‚â• 3.8  
- PyTorch (CPU o CUDA compatibile)  
- `tokenizers`, `transformers` (fallback tokenizer GPT-2)  
- `gradio`, `flask`, `pandas`, `psutil`, `ftfy`  
- `sqlite3` (builtin)

### Opzionali
- `whisper` ‚Üí trascrizione vocale  
- `gTTS` ‚Üí sintesi vocale  
- `pygame` ‚Üí riproduzione audio locale  

---

## üß© Installazione

### 1Ô∏è‚É£ Clona il repository
```bash
git clone https://github.com/Zingri/NanoGPTMarinelloWithJetsonSupport.git
cd NanoGPTMarinelloWithJetsonSupport
2Ô∏è‚É£ Crea e attiva l‚Äôambiente virtuale
bash
Copia codice
python -m venv venv
source venv/bin/activate   # Linux/macOS
venv\Scripts\activate      # Windows
3Ô∏è‚É£ Installa le dipendenze
bash
Copia codice
pip install -r requirements.txt
üìÅ File principali
File	Descrizione
NanoGPT_Marinello.py	Core del progetto: modello GPT, training, UI Gradio, DB conversazioni
LlmServer.py	Server Flask con API REST
checkpoint.pth	Checkpoint del modello salvato
tokenizer.json	Tokenizer BPE personalizzato
chat_memory.db	Database SQLite delle conversazioni
train_tokenizer.py	Script per generare un nuovo tokenizer

Se tokenizer.json non √® presente, esegui:

bash
Copia codice
python train_tokenizer.py
üí¨ Interfaccia Gradio
Avvia con:

bash
Copia codice
python NanoGPT_Marinello.py
Gradio sar√† disponibile su: http://127.0.0.1:7860

Funzionalit√† principali
Chat testuale o vocale (microfono/textbox)

Training interattivo del modello

Visualizzazione cronologia conversazioni

Informazioni di sistema (CPU/RAM/GPU/Whisper status)

üåê Server REST (per Jetson / integrazione remota)
Avvia con:

bash
Copia codice
python LlmServer.py
Server attivo su 0.0.0.0:5000

Endpoint principali
üîπ POST /api/train
Body JSON:

json
Copia codice
{"training_text": "...", "steps": 1000}
Risposte:

‚úÖ 200 OK: {"complete": "training completed"}

‚ùå 400/500: {"error": "..."}

Nota: testi inferiori a 100 caratteri vengono rifiutati.

üîπ POST /api/chat
Body JSON:

json
Copia codice
{"question": "...", "temperature": 0.7, "history": []}
Risposta:

json
Copia codice
{"response": "testo generato"}
La gestione della cronologia √® basata sul database interno.

üîπ GET /api/load
Risposta:

json
Copia codice
{
  "overview": [
    {
      "conversation_id": 1,
      "last_user": "...",
      "last_assistant": "..."
    }
  ]
}
üß† Uso su Jetson (Modalit√† Ibrida)
Avvia il server LLM sul PC principale:

bash
Copia codice
python LlmServer.py
Sul Jetson, esegui l‚Äôinterfaccia grafica:

bash
Copia codice
python GUI.py
Modifica la variabile NOTEBOOK_API_URL nel codice Jetson inserendo l‚ÄôIP del PC principale.
Questo consente la connessione tra GUI e server remoto.

üß™ Debug & Note operative
Problema	Soluzione
‚ö†Ô∏è Model not loaded	Esegui il training o verifica la presenza di checkpoint.pth.
üí• CUDA Out of Memory	Riduci batch_size o il numero di layer, oppure esegui su CPU.
üîá Whisper non disponibile	La trascrizione verr√† ignorata, la chat testuale funzioner√† comunque.

üí° Suggerimenti
Su GPU con VRAM limitata, riduci n_layer o lascia che get_adaptive_config() adatti automaticamente i parametri.

Su Jetson, utilizza versioni leggere di Whisper (tiny o small) e disattiva TTS se necessario.

Se esponi il server in rete, implementa autenticazione e rate-limiting per maggiore sicurezza.

üìö Riferimenti
Progetto sviluppato nell‚Äôambito della tesi:

Matteo Zingrillara (2025)
Assistenti LLM per l‚ÄôInterazione con i Sistemi Autonomi
Universit√† degli Studi di Padova

Repository di origine:

https://github.com/FrannPizz/NanoGPT-Marinello

Tesi di riferimento:

Francesco Pizzato (2025)
Conversing with Robots: Building LLM Assistants to Understand and Utilize Autonomous Systems
Universit√† degli Studi di Padova
